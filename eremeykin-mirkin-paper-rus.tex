%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Eremeykin and Mirkin paper for "Avtomatika i Telemekhanika"               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{a&t}
\usepackage{graphicx}

\begin{document}  %%%!!!

\year{2018}
\title{СОКРАЩЕНИЕ ВРЕМЕНИ ВЫЧИСЛЕНИЙ В АГЛОМЕРАТИВНОМ КЛАСТЕР-АНАЛИЗЕ ПЕРЕХОДОМ К ПОДВЫБОРКАМ}
%\thanks{Работа выполнена при финансовой поддержке \dots
%(грант \mbox{№\,\dots}).}

\authors{П.А.~ЕРЕМЕЙКИН, студент~НИУ~ВШЭ\\
Б.Г.~МИРКИН, док.~техн.~наук\\
(Национальный исследовательский университет\\ <<Высшая школа экономики>>, Москва)}

\maketitle

\begin{abstract}
Краткая аннотация статьи или заметки. Иногда не бывает. 
\begin{enumerate}
	\item Написать аннотацию
	\item Проверить ссылки на литературу и добавить в библиографию недостающие
\end{enumerate}
\end{abstract}


\section{Введение}

Методы кластеризации широко применяются для выявления структуры данных и поиска характерных групп объектов. По принадлежности заданного нового объекта к определённому кластеру можно сделать предположения о ключевых свойствах этого объекта. В общем случае под кластеризацией понимают  поиск в заданном множестве  непересекающихся однородных подмножеств, которые включают в себя подобные объекты \cite{Mirkin-Vvedenie-v-analiz-dannyh}.
 
Наиболее широко известный и часто применяемый метод кластеризации --- \mbox{k-means} \cite{K-Means-canonical}. Этот метод состоит в попеременной минимизации квадратичного критерия по двум типам переменных: центрам кластеров и принадлежности объектов кластеру. Критерии минимизации аналогичного вида выражают смысл некоторых популярных иерархических алгоритмов, например, Ward \cite{Ward-canonical} или Bisecting \mbox{K-Means}\cite{Mirkin-Clustering-A-Data-Recovery-Approach}.

Несмотря на популярность алгоритма \mbox{k-means}, он обладает существенными недостатками. Во-первых, для работы необходимо явно задать число кластеров. В практических приложениях подлинное число кластеров как правило неизвестно. Во-вторых, \mbox{k-means} требует правильной инициализации начального состояния, от которого существенным образом зависит результат. И, наконец, алгоритм чувствителен к шуму в данных, то есть он не предусматривает никакого механизма учёта возможных погрешностей, которые зачастую возникают в реальных данных, полученных при помощи измерений.

За свою богатую историю алгоритм \mbox{k-means} получил множество усовершенствований, модификаций и новых применений. В частности, существенным вкладом в развитие алгоритма стала работа \cite{Anomalous-Clustring}, в которой предложен так называемый метод аномальных кластеров. Метод аномальных кластеров позволяет рационально определить начальное состояние для алгоритма \mbox{k-means} путём поочерёдного выявления и исключения кластеров, наиболее удалённых от центра данных, называемых аномальными. 

Иерархические алгоритмы, основанные на \mbox{k-means}, в основном наследуют его недостатки, но привносят важное свойство: в ходе работы они выявляют дерево вложенности кластеров, которое может быть естественным образом использовано в некоторых приложениях. Например, такое дерево может отражать  филогенетическое родство при кластеризации биологических видов. Это интересное свойство побуждает исследователей искать пути устранения недостатков иерархических алгоритмов, использующих квадратичный критерий. 

Как показали результаты экспериментов, проведённых в работе \cite{Amorim-Makarenkov-Mirkin}, метод аномальных кластеров в большинстве случаев порождает избыточное число кластеров, поэтому его можно использовать как предварительный шаг для агломеративного алгоритма Ward. Алгоритм Ward исходит из представления о том что на начальном этапе всякий единичный объект выступает в роли отдельного кластера и на каждом шаге происходит объединение двух ближайших кластеров пока их общее число не достигнет заданного значения.  Несмотря на то, что Ward в каноническом виде не требует инициализации, применение описанного предварительного шага вызвано необходимостью повышения производительности. При большом количестве объектов, на первых итерациях алгоритма требуется выполнить большое число сравнений (квадратично зависящее от числа объектов), и следовательно, время работы алгоритма в этом случае недопустимо велико. Благодаря применению предварительной проработки кластерной структуры с помощью аномального анализа отпадает необходимость сравнивать большое число кластеров в которых содержится всего по одному или несколько объектов. 

Описанная выше модификация получила название \mbox{A-Ward}. В той же работе \cite{Amorim-Makarenkov-Mirkin} предложено дальнейшее усовершенствование алгоритма \mbox{A-Ward}, которое акцентирует внимание на обработке зашумлённых данных. Обобщённая версия алгоритма для произвольной степени Минковского $ p $ и с использованием весовых коэффициентов признаков $ w $, определяемых отдельно для каждого кластера, была названа \mbox{A-Ward$ _{p\beta} $}, где $ \beta $ обозначает степень весовых коэффициентов $ w $.

Численные эксперименты на синтетических и реальных данных показали высокую эффективность алгоритма \mbox{A-Ward$ _{p\beta} $} в том числе для случаев с большой зашумленностью признаков, поэтому алгоритм представляет интерес для применения в практических случаях. Тем не менее, ввод новых параметров $ p $ и $ \beta $ породил необходимость выработки методики для определения их значений. В статье \cite{SW-Based-Search} рассмотрен подход к определению $ p $ и $ \beta $ методом перебора, в котором критерием качества результата служит эмпирическая характеристика Silhouette Width (SW)\cite{SW-Canonical}. Такой подход поглощает преимущество в производительности, достигнутое введением предварительного шага аномального кластер-анализа, и требует больших временных затрат.

Специалистам по анализу данных хорошо известна идея, которая лежит в основе принципа кросс-валидации \cite{None} и заключается в сохранении основных свойств выборки даже в случае исключения из неё некоторой части объектов. Та же идея предположительно может быть использована для определения рациональных значений $ p $ и $ \beta $ по подвыборкам. Таким образом, в данной статье будет экспериментально исследована возможность выбора параметров $ p $ и $ \beta $ для алгоритма \mbox{A-Ward$ _{p\beta} $} путём перехода к подвыборкам и с использованием характеристики SW в качестве критерия качества результата.

\section{Предлагаемое решение}
\subsection{Описание эксперимента}
Пусть задано множество $ Y $ из $ N $ объектов, каждый из которых обладает $ V $ признаками. Такое множество можно выразить в виде таблицы данных:
\begin{gather} \label{eq:data-table}
	Y= \begin{pmatrix} 
	y_{1} \\
	\cdots \\ 
	y_{N} 
	\end{pmatrix}
	= \begin{pmatrix} 
		y_{11} & \cdots  & y_{1V} \\ 
		\cdots & \cdots  & \cdots \\ 
		y_{N1} & \cdots  & y_{NV} 
	\end{pmatrix}
\end{gather}
Алгоритм \mbox{A-Ward$ _{p\beta} $} при заданных значениях параметров $ p $ и $ \beta $  позволяет получить кластерное разбиение $ S=\{C_1, \ldots, C_K\} $  этого множества. Получаемое кластерное разбиение состоит из заранее определённого числа $ K $ непересекающихся кластеров $ C_k $, объединение которых покрывает всё множество объектов $ Y $. Принцип работы и формальное описание алгоритма \mbox{A-Ward$ _{p\beta} $} приведены в разделе \ref{sec:a-ward-p-beta}.

Для произвольного множества $ Y $ до сих про не было сформулировано эффективной методики поиска параметров  $ p $ и $ \beta $. Как было отмечено во введении, в статье \cite{SW-Based-Search} был опробован способ выбора параметров путём многократного запуска алгоритма \mbox{A-Ward$ _{p\beta} $} при переборе значений параметров в диапазоне $ \lbrack 1,5 \rbrack $ с шагом 0.1. Указанный диапазон определён исходя из опыта применения алгоритма: с превышением значений параметров 5 не достигается существенного улучшения качества разбиения. Нижняя граница диапазона определена математическим смыслом параметров.  

Описанный способ потребует $ 41^2= 1681 $ запуск алгоритма, что чрезвычайно затратно с точки зрения времени. Поэтому была предложена идея для определения значений параметров по случайным подвыборкам. Из всего множества $ Y $ выбирается $ L $ подмножеств $ Y_l \subset Y $ с заданным числом элементов $ n \ll N $. По каждому подмножеству  $ Y_l $ оцениваются рациональные значения параметров $ p^*_l $ и $ \beta^*_l $. Усреднённые по $ l $ значения $ p^* = \overline{p^*_l} $ и $ \beta^* = \overline{\beta^*_l} $ принимаются в качестве рационального выбора для всего множества $ Y $. При правильном выборе соотношения между числом объектов в полной выборке $ N $, числом объектов в подвыборке $ n $, и числом подвыборок $ M $, как ожидается, можно получить результат кластеризации близкий к результату, полученному с помощью оценки по всей выборке $ N $, затратив при этом существенно меньшее время.

Для подтверждения вышеописанных предположений предлагается рассмотреть численный эксперимент на синтетических данных. Синтетическая генерация позволяет гибко изменять характеристики данных, например число признаков и количество кластеров, степень их взаимного смешивания, а также определять истинное разбиение. Для генерации данных применяется метод описанный в статье \cite{Kovaleva}. Подробно его принцип работы разобран в разделе \ref{sec:data-generation}. Сейчас стоит иметь ввиду, что результат работы данного генератора синтетических данных полностью определяется следующими пятью параметрами:
\begin{enumerate}
	\item
	Число объектов $ N $
	\item
	Число признаков $ V $
	\item
	Количество кластеров $ K $
	\item 
	Минимальное число объектов в кластере $ m $
	\item
	Степень взаимного смешивания кластеров $ a \in \lbrack0,1\rbrack $
\end{enumerate}

Данные сгенерированные по методике, описанной в \cite{Kovaleva} мы будем обозначать ключевым словом \texttt{kovaleva}, за которым через подчёркивание следуют обозначения размерности (например \mbox{$\mathtt{1000 \times 15 } $}), общего числа кластеров с префиксом \texttt{с}, минимального числа объектов в кластере с префиксом \texttt{m} и, наконец, степени взаимного смешивания с префиксом \texttt{a}. В таблице \ref{tab:datasets} приведены три типа данных которые будут использованы в ходе эксперимента с пояснениями в принятых обозначениях. Данные, сгенерированные по указанным значениям параметров, обладают важным свойством: они соответствуют трём случаям, взаимного отношения числа признаков и количества кластеров. В первом случае число кластеров меньше числа признаков, во втором случае примерно равно, а в третьем --- больше.

\begin{table}[h!]
	\centering
	\caption{Параметры данных} \label{tab:datasets}
	\begin{tabular}{ |l|c|c|c|c|c| }
					\hline Обозначение               & N & V & K & m & a\\ 
					\hline $\mathtt{kovaleva\_1000 \times 15\_c7\_m100\_a0.5}$ & 1000 & 15 & 7 & 100 & 0.5\\ 
					\hline $\mathtt{kovaleva\_1000 \times 15\_c12\_m60\_a0.5}$ & 1000 & 15 & 12 & 60 & 0.5\\ 
					\hline $\mathtt{kovaleva\_1000 \times 15\_c19\_m35\_a0.5}$ & 1000 & 15 & 19 & 35 & 0.5\\ 
					\hline
	\end{tabular}
\end{table}

В ходе эксперимента будет рассмотрены четыре схем формирования подвыборки (без учёта полной выборки как эталонного случая). Первые две схемы представляют собой однократный выбор соответственно по 100 и 200 объектов, два оставшиеся~--- пятикратное формирование подвыборок по 100 и 200 объектов с последующим усреднением результата. В таблице \ref{tab:subsampling} приведены все пять рассматриваемых схем. 

\begin{table}[h!]
	\centering
	\caption{Схемы формирования подвыборки} \label{tab:subsampling}
	\begin{tabular}{ |l|c|c| }
		\hline Схема формирования подвыборки & \begin{tabular}{@{}c@{}}Размер подвыборки \\ $ n $\end{tabular} & \begin{tabular}{@{}c@{}}Число подвыборок  \\ $ L $\end{tabular}\\ 
		\hline Полная выборка (1-1000) & 1000 & 1 \\ 
		\hline Однокрантый выбор по 100 (1-100) & 100 & 1 \\ 
		\hline Однокрантый выбор по 200 (1-200) & 200 & 1 \\ 
		\hline Пятикрантый выбор по 100 (5-100) & 100 & 5 \\ 
		\hline Пятикрантый выбор по 200 (5-200) & 200 & 5 \\ 		
		\hline
	\end{tabular}
\end{table}

При применении алгоритма \mbox{A-Ward$ _{p\beta} $} на реальных данных для оценки качества результата и выбора наилучших значений  $ p $, $ \beta $  нет возможности использовать подлинное разбиение, поэтому популярный индекс ARI \cite{ARI-canonical} не подходит для применения в роли целевой характеристики. В качестве замены этому индексу может быть использована эмпирическая величина Silhouette Width (SW)\cite{SW-Canonical}, хорошо зарекомендовавшая себя во многих приложениях \cite{SW1,SW2}. Интересно также установить насколько использование эмпирической величины SW ухудшает результат относительно результата, полученного с использованием подлинного разбиения и индекса ARI. Синтетическая генерация данных позволит применить индекс ARI для оценки разбиений, получаемых при различных значениях параметров $ p $, $ \beta $, относительно истинного разбиения. Способы вычисления характеристики SW и индекса ARI описаны в разделах \ref{sec:sw} и \ref{sec:ari} соответственно.

При заданной характеристике качества может возникнуть ситуация неоднозначности при которой максимум достигается для нескольких пар параметров  $ p $ и $ \beta $. Для разрешения этой неоднозначности в рамках эксперимента предлагается рассмотреть два подхода:
\begin{enumerate}
	\item
	Предпочтение отдаётся паре параметров $ p $, $ \beta $ для которой величина $ p^2+\beta^2 $ минимальна
	\item
	Пусть $ \{(p_t, \beta_t): t=1,\ldots,T\} $ --- множество пар параметров для которых достигается максимум выбранной характеристики качества (SW или ARI). Рассчитаем поэлементные средние значения $ \overline{p} = \dfrac{1}{T}\sum_{t=1}^{T}p_t,\:\overline{\beta} = \dfrac{1}{T}\sum_{t=1}^{T}\beta_t $. Наилучшим выбором признается та пара, для которой значение $ (p_t-\overline{p})^2 + (\beta_t-\overline{\beta})^2 $ минимально.
\end{enumerate}

Ниже приведена формулировка алгоритма подбора параметров $ p $, $ \beta $, используемого в эксперименте. Описанный алгоритм применяется для синтетических данных из таблицы \ref{tab:datasets} c использованием в общей сложности пяти схем формирования подвыборки согласно таблице \ref{tab:subsampling}. При этом рассматриваются две характеристики качества разбиения: эмпирическая SW и индекс ARI, основанный на известном истинном разбиении. Для разрешения неопределённости выбора максимума характеристики рассматриваются два подхода, описанных в списке выше.

\begin{algorithm}[(Подбор параметров $ p $, $ \beta $ с переходом к подвыборкам)] \label{alg:experiment}
	\
	\begin{enumlist}[.] 
		\item
		Из множества $ Y $ случайным образом выбрать $ L $ подмножеств $ Y_l \subset Y $ с заданным размером $ n $. Пересечение выбранных подмножеств допускается.
		
		\item
		Для всех значений параметров $ p=1,1.1,\ldots,5 $ и $ \beta=1,1.1,\ldots,5 $ выполнить алгоритм \mbox{A-Ward$ _{p\beta} $} применительно к каждому подмножеству  $ Y_l $ и получить результирующее разбиение $ S_l^{p\beta} $.
		
		\item
		По каждому полученному разбиению  $ S_l^{p\beta} $ рассчитать значение характеристики Silhouette Width (или ARI): $ SW_l^{p\beta} = SW(S_l^{p\beta}) $.
		
		\item
		Для каждого подмножества $ Y_l $ выбрать рациональные значения параметров $ p $ и $ \beta $, соответствующие максимальным значениям характеристики Silhouette Width (или ARI): $ (p^*_l,\beta^*_l)=\operatorname{arg\,max}\{SW_l^{p\beta}\} $. В случае, если максимум достигается для нескольких пар параметров, рассмотреть два возможных подхода разрешения неоднозначности. 
		
		\item Результатом работы алгоритма являются усреднённые по $ l $ значения параметров $ p^* = \overline{p^*_l} $ и $ \beta^* = \overline{\beta^*_l} $. 
		
	\end{enumlist}
\end{algorithm}

\subsection{Алгоритм \mbox{A-Ward$ _{p\beta} $}} \label{sec:a-ward-p-beta}

Рассмотрение алгоритма \mbox{A-Ward$ _{p\beta} $} следует начинать с описания общего принципа работы иерархических алгоритмов и, в частности, Ward. В отличие от неиерархических алгоритмов, которые формируют плоскую структуру кластеров, как, например, \mbox{k-means}, иерархические алгоритмы в качестве результата предоставляют дополнительную информацию относительно взаимосвязей между кластерами. Эта информация выражена в виде вложенной последовательности разбиений \cite{Amorim-Makarenkov-Mirkin} и графически может быть изображена с помощью дендрограммы. На каждом уровне дендрограммы определённый объект $ y_i \in Y$ принадлежит единственному кластеру $ C_k $. В то же время этот объект может принадлежать на других уровнях другим кластерам, которые образованы разбиением кластера $ C_k $ или слиянием с другими кластерами. 

Различают два вида иерархический  алгоритмов: агломеративные (объединяющие) и дивизивные (разделяющие). Агломеративные алгоритмы работают по принципу ``снизу верх''. На начальном этапе работы таких алгоритмов каждый единичный объект выступает в роли кластера. Во время работы происходит попарное объединение кластеров до тех пор, пока не будет достигнуто заданное число кластеров. Дивизивные алгоритмы в противоположность агломеративным действуют ``сверху вниз''.

\begin{algorithm}[(Инициализация аномальными кластерами)] \label{alg:anomalous-p-beta}
	\
	\begin{enumlist}[.] 
		
		\item \textit{Инициализация.} Задаться значениями параметров $ p $ и $ \beta $. Глобальный центр данных $ c_Y $ вычислить как покомпонентный центр Минковского по всем объектам \mbox{$ y_i \in Y $.}
		
		\item \label{itm:anomalous-p-beta-tent-centroid} \textit{Текущий центр.} Задать пустой аномальный кластер $ C_t = \varnothing $. Веса равномерно распределить по всем признакам $ w_{kv} = 1/V $ при $ k=1,2 $ и $ v=1,\ldots,V $. Текущий центр аномального кластера $ c_t $ выбрать как объект, наиболее удалённый от глобального центра $ c_Y $. Расстояние между объектом $ y_i $ и центром произвольного кластера $ c_k $ вычисляется по формуле:
		\begin{gather} \label{eq:obj-to-centroid-distance}
			d_{p\beta}(y_i,c_k)=\sum_{v=1}^{V}w^\beta_{kv}|y_{iv}-c_{kv}|^p
		\end{gather}
		
		\item \textit{Формирование аномального кластера.} В аномальный кластер добавить объекты, которые расположены ближе к текущему центру аномального кластера $ c_t $, чем к глобальному центру $ c_Y $ согласно расстоянию, определяемому формулой \ref{eq:obj-to-centroid-distance}. Если изменений в разбиении нет, прейти к шагу \ref{itm:anomalous-p-beta-save}.
		
		\item \textit{Обновление текущего центра.} Вычислить текущий центр аномального кластера $ c_t $ как покомпонентный центр Минковского по всем объектам в аномальном кластере $ y_i \in C_t $.
		
		\item \textit{Обновление весов.} Вычислить веса признаков по следующей формуле:
		\begin{gather} \label{eq:weights}
			w_{kv}=\dfrac{1}{\sum_{u=1}^{V} \left ( \dfrac{D_{kv}}{D_{ku}} \right )^{\frac{1}{\beta-1}}},
		\end{gather}
		\begin{tabular}{llll}
			где & $D_{kv}=\sum_{i\in C_k}^{}|y_{iv}-c_{kv}|^\beta$ & {---} & разброс признака $ v $ в кластере $ C_k $ \\
		\end{tabular}
		
		\item \label{itm:anomalous-p-beta-save} \textit{Сохранение параметров.} Включить текущий центр аномального кластера $ c_t $ в список центров $ \mathtt{c\_list} $, а веса $ w $ в список весов $ \mathtt{w\_list} $.
		
		\item \textit{Исключение аномального кластера.} Исключить из $ Y $ все объекты $ y_i \in C_t $. Если $ Y \neq \varnothing $, перейти к шагу \ref{itm:anomalous-p-beta-tent-centroid}.			
		
		\item \textit{ Результат.} Результатом работы алгоритма является разбиение S, а также списки центров кластеров $ \mathtt{c\_list} $ и весов $ \mathtt{w\_list} $.
	\end{enumlist}
\end{algorithm}

\begin{algorithm}[(\mbox{$ imwk$-means$_{p\beta} $})] \label{alg:imwk-means}
	\
	\begin{enumlist}[.] 
		
		\item \textit{Инициализация.} Установить текущее разбиение пустым $ S=\varnothing $, а число кластеров $ K $ равным длине списка $ \mathtt{c\_list} $, который был получен при аномальной инициализации. 
		
		\item \label{itm:imwl-means-assignment}\textit{Формирование кластеров.} Каждый объект $ y_i \in Y $ поместить в кластер, центр которого $ c_k $ находится ближе всего к этому объекту. Близость объекта к центру кластера определяется  по формуле \ref{eq:obj-to-centroid-distance}. Если нет изменений в разбиении $  S $, перейти к шагу \ref{itm:imwl-means-result}.
		
		\item \textit{Обновление центров.} Вычислить новые координаты центра $ c_k $ каждого кластера $ C_k $ как покомпонентный центр Минковского всех объектов этого кластера $ y_i \in C_k $.
		
		\item \textit{Обновление весов.} Вычислить новые веса $ w_{kv} $ по формуле \ref{eq:weights} для $ k=1,\ldots,K $ и $ v=1,\ldots,V $. Перейти к шагу \ref{itm:imwl-means-assignment}
		
		\item \label{itm:imwl-means-result} \textit{ Результат.} Результатом работы алгоритма является разбиение S, а также списки центров кластеров $ \mathtt{c\_list} $ и весов $ \mathtt{w\_list} $.
	\end{enumlist}
\end{algorithm}

\begin{algorithm}[(\mbox{A-Ward$ _{p\beta} $})] \label{alg:a-ward-p-beta}
	\
	\begin{enumlist}[.] 
		\item \textit{Инициализация.} Параметры $ p $ и $ \beta $ остаются неизменными, которые были определены для \mbox{$ imwk$-means$_{p\beta} $}. Начальное состояние соответствует конечному для \mbox{$ imwk$-means$_{p\beta} $}: исходный список центров кластеров $ \mathtt{c\_list} $ и весов $ \mathtt{w\_list} $ является результатом работы предыдущего этапа.
		
		\item \label{itm:a-ward-p-beta-merge} \textit{Объединение кластеров.} Выбрать два ближайших кластера $ C_a,\:C_b \in S$ и объединить их в новый $ C_{ab} $. Близость кластеров определяется по следующей формуле: 
		\begin{gather} \label{eq:ward-p-beta-distance}
		d_{Ward}(C_a,C_b)=\dfrac{N_aN_b}{N_a+N_b}\sum_{v=1}^{V}\left ( \frac{w_{av}+w_{bv}}{2} \right )^\beta |c_{av}-c_{bv}|^p,
		\end{gather}
		
		\begin{tabular}{llll}
			где & $ N_a,\:N_b $ & {---} & количество объектов в кластерах $ C_a $ и $ C_b $ соответственно \\
			& $ V $ & {---} & число признаков у каждого объекта $ y_i \in Y $ \\
			& $w_{av},\:w_{bv} $ & {---} & веса $ v $-го признака в кластере $ C_a $ и $ C_b $ соответственно \\
			& $ c_{av},\:c_{bv} $ & {---} & $ v $-ая координата центров кластеров $ C_a $ и $ C_b $ соответственно 
		\end{tabular}
		
		\item \textit{Обновление центра.} Вычислить новое значение центра $ C_{ab} $ как покомпонентный центр Минковского по всем объектам $ y_i \in C_{ab} $.
		
		\item \textit{Обновление весов.} Вычислить новые веса $ w_{kv} $ по формуле \ref{eq:weights} для $ k=1,\ldots,K $ и $ v=1,\ldots,V $.
		
		\item \textit{Условие остановки.} Уменьшить текущее число кластеров на единицу. Если текуще число кластеров все ещё больше единицы или требуемого числа кластеров, прейти к шагу \ref{itm:a-ward-p-beta-merge} .
		
		
		
	\end{enumlist}
\end{algorithm}


\newpage
Исходя из описания алгоритма Ward видно, что на начальных этапах работы происходит сравнение большого числа кластеров, примерно равного числу объектов. 


Алгоритм Ward относится к агломеративным. На каждой итерации происходит слияние ближайших кластеров, таким образом чтобы внутрикластерная дисперсия была минимальной. Близость кластеров определяется по формуле:

\begin{gather} \label{eq:ward-distance}
	d_{Ward}(C_a,C_b)=\dfrac{N_aN_b}{N_a+N_b}\sum_{v=1}^{V}(c_{av}-c_{bv})^2,
\end{gather}

\begin{tabular}{llll}
	где & $ C_a,\:C_b $ & {---} & два произвольных кластера, \\
	& $ N_a,\:N_b $ & {---} & количество объектов в кластерах $ C_a $ и $ C_b $ соответственно \\
	& $ V $ & {---} & число признаков у каждого объекта $ y_i \in Y $ \\
	& $ c_{va},\:c_{vb} $ & {---} & $ v $-ая координата центров кластеров $ C_a $ и $ C_b $ соответственно 
\end{tabular}

В традиционной постановке алгоритм Ward формулируется следующим образом:




%\begin{algorithm}[(Ward)] \label{alg:experiment}
%	\
%	\begin{enumlist}[.] 
%		\item \textit{Инициализация.}
%		Задаться требуемым количеством кластеров $ K $. Установить текущее число кластеров $ k=N $ и текущее разбиение $ S=\{C_1,\ldots,C_N\} $, где каждый кластер состоит из единственного объекта $ C_k=\{y_k\} $. Центр кластера совпадает с объектом, входящим в этот кластер.
%		\item \label{itm:merge}\textit{Слияние.}
%		Определить два ближайших кластера $ C_a,\:C_b $, используя формулу \ref{eq:ward-distance}. Произвести слияние найденных кластеров в результирующий кластер $ C_{ab} = C_a \cup C_b $, содержащий одновременно объекты  $ C_a $ и $ C_b $.
%		\item \textit{Обновление центра.}
%		Установить центр кластера $ C_{ab} $ равным покомпонентному среднему всех объектов, входящих в $ C_{ab} $.
%		\item \textit{Условие остановки.}
%		Уменьшить текущее количество кластеров $ k $ на единицу. Если $ k>1 $ или $ k>K $, перейти к шагу \ref{itm:merge}. В противном случае текущее разбиение $ S $ является результирующим.
%	\end{enumlist}
%\end{algorithm}
\newpage
\subsection{Генератор данных типа kovaleva}\label{sec:data-generation}
\subsection{Характеристика разбиения SW}\label{sec:sw}
\subsection{Характеристика разбиения ARI}\label{sec:ari}

данные 
способ оценки
что включено в сравнение
(со всеми деталями, включая АУорд, Силуэт Видт и пр.)


\section{Организация эксперимента}

\section{Результаты эксперимента}
С пояснениями и выводами

\section{Заключение}

Что сделано и куда двигаться.
\begin{enumlist} % перечни, нумеруемые 1) 2) и т.д.
	\item
	3 разных числа кластеров соответствуют числу признаков $ >,<,\approx $ признаков	
	\item
	SW работает не хуже ARI
	\item
	Сокращение по времени есть
\end{enumlist}

\begin{thebibliography}{10}


\bibitem{Mirkin-Vvedenie-v-analiz-dannyh}
{\it Миркин Б.Г.}
Введение в анализ данных. М.: Юрайт, 2015.

\bibitem{K-Means-canonical}
{\it Ball G.H., Hall D.J.}
A clustering technique for summarizing multivariate data, Behavioral Science. 1967. V. 12 Iss. 2 P. 153--155.

\bibitem{Ward-canonical}
{\it Joe H., Ward Jr.}
Hierarchical grouping to optimize an objective function, Journal of the American Statistical Association. 1963. V. 58 Iss. 301 P. 236--244.

\bibitem{Mirkin-Clustering-A-Data-Recovery-Approach}
{\it Mirkin B.}
Clustering: A Data Recovery Approach. London: CRC Press, 2012.

\bibitem{Anomalous-Clustring}
{\it Chiang M.M.-T., Mirkin B.}
Intelligent choice of the number of clusters in k-means clustering: an experimental study with different cluster spreads, Journal of Classification. 2010. V. 27 Iss. 1. 3--40.

\bibitem{Amorim-Makarenkov-Mirkin}
{\it de Amorim R.C., Makarenkov V., Mirkin B.}
\mbox{A-Ward$ _{p\beta} $}: Effective hierarchical clustering using the Minkowski metric and a fast \mbox{k-means} initialisation. Information Sciences. 2016. V. 370-371 P. 343--354.

\bibitem{SW-Based-Search}
{\it de Amorim R.C., Shestakov A., Mirkin B., Makarenkov V.}
The Minkowski central partition as a pointer to a suitable distance exponent and consensus partitioning. Pattern Recognition. 2017. V. 67 P. 62--72.

\bibitem{SW-Canonical}
{\it Rousseeuw P.}
Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics. 1987. V.20 P. 53--65.

\end{thebibliography}

\AdditionalInformation{Еремейкин П.А.}{Национальный исследовательский университет <<Высшая школа экономики>>, студент, Москва}{eremeykin@gmail.com}

\AdditionalInformation{Миркин Б.Г.}{Национальный исследовательский университет <<Высшая школа экономики>>, профессор, Москва}{bmirkin@hse.ru}



\end{document}
